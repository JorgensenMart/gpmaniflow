{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0086167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8dfd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uci_datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a6d68ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "housing dataset, N=506, d=13\n"
     ]
    }
   ],
   "source": [
    "data_name = \"protein\"\n",
    "data = Dataset(data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b388773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.4688  -11.364    -1.1268  ...  -0.65553  34.826     0.95694]\n",
      " [ -3.5078  -11.364    16.603   ...   1.6445   33.436     5.4169 ]\n",
      " [ -3.5633   23.636    -5.0768  ...  -1.5555   37.346    -0.22306]\n",
      " ...\n",
      " [ -3.5569  -11.364    -7.7268  ...  -0.65553  40.226    -7.1531 ]\n",
      " [  3.4091  -11.364     6.9632  ...   1.7445  -36.694     3.0469 ]\n",
      " [ -2.3401  -11.364     8.4432  ...  -3.7555  -17.754    -7.1531 ]]\n",
      "[[ -3.2328  ]\n",
      " [ -8.9328  ]\n",
      " [ -5.4328  ]\n",
      " [-17.533   ]\n",
      " [ -4.7328  ]\n",
      " [ -6.9328  ]\n",
      " [-12.333   ]\n",
      " [ 25.767   ]\n",
      " [-10.433   ]\n",
      " [ -5.1328  ]\n",
      " [  1.5672  ]\n",
      " [-10.033   ]\n",
      " [ -0.33281 ]\n",
      " [ -7.2328  ]\n",
      " [-14.133   ]\n",
      " [  1.0672  ]\n",
      " [ -0.032806]\n",
      " [ -3.1328  ]\n",
      " [  2.0672  ]\n",
      " [ 10.567   ]\n",
      " [  7.2672  ]\n",
      " [ -1.3328  ]\n",
      " [ -1.0328  ]\n",
      " [-14.033   ]\n",
      " [ -7.3328  ]\n",
      " [ -2.4328  ]\n",
      " [ -5.0328  ]\n",
      " [ 10.167   ]\n",
      " [  0.76719 ]\n",
      " [-14.233   ]\n",
      " [ -6.9328  ]\n",
      " [ -3.6328  ]\n",
      " [ -3.7328  ]\n",
      " [ -8.0328  ]\n",
      " [ -9.4328  ]\n",
      " [ -7.1328  ]\n",
      " [ -9.5328  ]\n",
      " [ -0.83281 ]\n",
      " [ -0.43281 ]\n",
      " [ -8.4328  ]\n",
      " [ -4.7328  ]\n",
      " [  9.1672  ]\n",
      " [ -0.53281 ]\n",
      " [ -1.6328  ]\n",
      " [ -1.9328  ]\n",
      " [-15.133   ]\n",
      " [ -8.7328  ]\n",
      " [  1.5672  ]\n",
      " [ -9.2328  ]\n",
      " [ -5.2328  ]\n",
      " [ -4.7328  ]\n",
      " [ -5.0328  ]\n",
      " [ -2.0328  ]\n",
      " [ -0.032806]\n",
      " [-11.033   ]\n",
      " [ -6.3328  ]\n",
      " [-12.133   ]\n",
      " [ -0.23281 ]\n",
      " [ -6.0328  ]\n",
      " [ -1.1328  ]\n",
      " [  2.5672  ]\n",
      " [ -3.2328  ]\n",
      " [  4.0672  ]\n",
      " [ -3.0328  ]\n",
      " [ -2.2328  ]\n",
      " [  9.9672  ]\n",
      " [ -7.1328  ]\n",
      " [ -1.0328  ]\n",
      " [ -8.6328  ]\n",
      " [ -3.0328  ]\n",
      " [  1.8672  ]\n",
      " [ -6.9328  ]\n",
      " [-14.433   ]\n",
      " [ 27.467   ]\n",
      " [  2.0672  ]\n",
      " [ -5.9328  ]\n",
      " [ 12.367   ]\n",
      " [  0.66719 ]\n",
      " [  4.5672  ]\n",
      " [  1.4672  ]\n",
      " [ -4.3328  ]\n",
      " [  4.9672  ]\n",
      " [  8.2672  ]\n",
      " [  9.0672  ]\n",
      " [ -1.3328  ]\n",
      " [  1.2672  ]\n",
      " [  0.067194]\n",
      " [  6.0672  ]\n",
      " [  2.1672  ]\n",
      " [-12.833   ]\n",
      " [ -1.9328  ]\n",
      " [  2.4672  ]\n",
      " [ -2.7328  ]\n",
      " [  7.0672  ]\n",
      " [ -3.6328  ]\n",
      " [ -8.2328  ]\n",
      " [  2.2672  ]\n",
      " [ -9.8328  ]\n",
      " [-10.833   ]\n",
      " [ -1.9328  ]\n",
      " [-15.333   ]\n",
      " [  7.2672  ]\n",
      " [ -6.5328  ]\n",
      " [  0.16719 ]\n",
      " [ -1.8328  ]\n",
      " [ -2.6328  ]\n",
      " [-14.033   ]\n",
      " [  5.9672  ]\n",
      " [-11.633   ]\n",
      " [ 19.767   ]\n",
      " [  0.46719 ]\n",
      " [  5.4672  ]\n",
      " [  0.86719 ]\n",
      " [ -3.4328  ]\n",
      " [ -0.53281 ]\n",
      " [ -8.5328  ]\n",
      " [  2.7672  ]\n",
      " [  1.1672  ]\n",
      " [ -0.13281 ]\n",
      " [  0.36719 ]\n",
      " [ -0.53281 ]\n",
      " [-10.733   ]\n",
      " [ -4.6328  ]\n",
      " [ -6.9328  ]\n",
      " [ -0.53281 ]\n",
      " [  0.26719 ]\n",
      " [  6.4672  ]\n",
      " [ -4.7328  ]\n",
      " [  2.6672  ]\n",
      " [  0.56719 ]\n",
      " [ -3.5328  ]\n",
      " [  1.3672  ]\n",
      " [-14.133   ]\n",
      " [ -9.4328  ]\n",
      " [ -9.4328  ]\n",
      " [ -2.8328  ]\n",
      " [ -7.9328  ]\n",
      " [  0.26719 ]\n",
      " [-10.633   ]\n",
      " [ 13.667   ]\n",
      " [  1.3672  ]\n",
      " [  0.66719 ]\n",
      " [  0.76719 ]\n",
      " [ -9.8328  ]\n",
      " [ -0.73281 ]\n",
      " [ -1.5328  ]\n",
      " [ -4.5328  ]\n",
      " [ -9.0328  ]\n",
      " [ 27.467   ]\n",
      " [ 12.567   ]\n",
      " [  0.46719 ]\n",
      " [  0.36719 ]\n",
      " [ -3.6328  ]\n",
      " [  0.76719 ]\n",
      " [ 17.267   ]\n",
      " [ -2.2328  ]\n",
      " [ 27.467   ]\n",
      " [ 21.467   ]\n",
      " [  1.4672  ]\n",
      " [  0.46719 ]\n",
      " [ -0.33281 ]\n",
      " [ 11.267   ]\n",
      " [ -1.1328  ]\n",
      " [ 10.367   ]\n",
      " [ -8.7328  ]\n",
      " [-10.633   ]\n",
      " [ -3.1328  ]\n",
      " [ -2.4328  ]\n",
      " [  6.5672  ]\n",
      " [-12.333   ]\n",
      " [ 23.467   ]\n",
      " [  9.8672  ]\n",
      " [ -4.0328  ]\n",
      " [ 12.667   ]\n",
      " [-12.033   ]\n",
      " [ -0.93281 ]\n",
      " [ -2.1328  ]\n",
      " [  0.96719 ]\n",
      " [ -4.3328  ]\n",
      " [ -5.1328  ]\n",
      " [ -0.032806]\n",
      " [ 27.467   ]\n",
      " [  6.1672  ]\n",
      " [ -7.6328  ]\n",
      " [ -2.5328  ]\n",
      " [ -3.1328  ]\n",
      " [ -3.7328  ]\n",
      " [ -2.6328  ]\n",
      " [  8.9672  ]\n",
      " [ -3.2328  ]\n",
      " [ -2.0328  ]\n",
      " [  0.56719 ]\n",
      " [ 27.467   ]\n",
      " [  8.1672  ]\n",
      " [  1.2672  ]\n",
      " [ -5.5328  ]\n",
      " [-10.533   ]\n",
      " [ -2.6328  ]\n",
      " [  5.8672  ]\n",
      " [ -9.2328  ]\n",
      " [ 10.767   ]\n",
      " [  1.7672  ]\n",
      " [-12.333   ]\n",
      " [ 27.467   ]\n",
      " [ -5.9328  ]\n",
      " [ -5.0328  ]\n",
      " [ -2.2328  ]\n",
      " [  0.56719 ]\n",
      " [ 27.467   ]\n",
      " [ 27.467   ]\n",
      " [ 20.967   ]\n",
      " [  1.9672  ]\n",
      " [ -6.8328  ]\n",
      " [ 27.467   ]\n",
      " [  7.7672  ]\n",
      " [ -7.4328  ]\n",
      " [ 14.767   ]\n",
      " [ 12.867   ]\n",
      " [ -7.6328  ]\n",
      " [ -8.0328  ]\n",
      " [ -3.0328  ]\n",
      " [ -1.1328  ]\n",
      " [ -1.3328  ]\n",
      " [ 13.467   ]\n",
      " [ -3.1328  ]\n",
      " [-13.733   ]\n",
      " [  0.16719 ]\n",
      " [ -1.3328  ]\n",
      " [ 10.667   ]\n",
      " [  6.1672  ]\n",
      " [ -0.93281 ]\n",
      " [  0.067194]\n",
      " [ -7.3328  ]\n",
      " [ -2.9328  ]\n",
      " [  0.66719 ]\n",
      " [  4.9672  ]\n",
      " [ 19.167   ]\n",
      " [  2.4672  ]\n",
      " [  2.2672  ]\n",
      " [  1.9672  ]\n",
      " [  1.7672  ]\n",
      " [-15.533   ]\n",
      " [-12.033   ]\n",
      " [-15.533   ]\n",
      " [  3.6672  ]\n",
      " [ 20.567   ]\n",
      " [  1.3672  ]\n",
      " [ -2.0328  ]\n",
      " [  0.56719 ]\n",
      " [ -0.73281 ]\n",
      " [ -4.1328  ]\n",
      " [  5.6672  ]\n",
      " [ 26.267   ]\n",
      " [  0.66719 ]\n",
      " [ 15.367   ]\n",
      " [ -5.3328  ]\n",
      " [  6.5672  ]\n",
      " [  7.0672  ]\n",
      " [ -2.9328  ]\n",
      " [ 18.767   ]\n",
      " [ -7.5328  ]\n",
      " [ -2.6328  ]\n",
      " [ -1.7328  ]\n",
      " [  0.56719 ]\n",
      " [ -0.83281 ]\n",
      " [ -8.0328  ]\n",
      " [ -3.1328  ]\n",
      " [  2.1672  ]\n",
      " [  1.2672  ]\n",
      " [-17.533   ]\n",
      " [ -2.5328  ]\n",
      " [ 12.367   ]\n",
      " [ -6.4328  ]\n",
      " [ -9.1328  ]\n",
      " [ -8.9328  ]\n",
      " [ -4.9328  ]\n",
      " [ -6.4328  ]\n",
      " [ -2.5328  ]\n",
      " [ 13.867   ]\n",
      " [ -2.1328  ]\n",
      " [ -5.4328  ]\n",
      " [ -2.4328  ]\n",
      " [  7.3672  ]\n",
      " [  9.6672  ]\n",
      " [ -4.0328  ]\n",
      " [  0.26719 ]\n",
      " [ -3.4328  ]\n",
      " [ -2.9328  ]\n",
      " [ -2.7328  ]\n",
      " [  7.5672  ]\n",
      " [ -4.0328  ]\n",
      " [  7.9672  ]\n",
      " [ -1.9328  ]\n",
      " [ -3.2328  ]\n",
      " [-16.233   ]\n",
      " [  8.9672  ]\n",
      " [ -3.3328  ]\n",
      " [ 20.267   ]\n",
      " [ -4.4328  ]\n",
      " [  1.3672  ]\n",
      " [  4.9672  ]\n",
      " [ -3.4328  ]\n",
      " [ -9.0328  ]\n",
      " [  1.6672  ]\n",
      " [ -2.4328  ]\n",
      " [ -4.8328  ]\n",
      " [-16.933   ]\n",
      " [ -7.9328  ]\n",
      " [ -0.83281 ]\n",
      " [ -0.63281 ]\n",
      " [-14.233   ]\n",
      " [ 22.267   ]\n",
      " [  2.1672  ]\n",
      " [  8.4672  ]\n",
      " [ -1.4328  ]\n",
      " [ -0.23281 ]\n",
      " [ -0.13281 ]\n",
      " [  4.1672  ]\n",
      " [ 22.867   ]\n",
      " [ -2.4328  ]\n",
      " [ -3.9328  ]\n",
      " [ 12.367   ]\n",
      " [  4.9672  ]\n",
      " [ -7.7328  ]\n",
      " [ -6.1328  ]\n",
      " [-11.533   ]\n",
      " [  2.4672  ]\n",
      " [ 10.467   ]\n",
      " [  8.6672  ]\n",
      " [  4.0672  ]\n",
      " [ -0.83281 ]\n",
      " [ 16.167   ]\n",
      " [-11.733   ]\n",
      " [ -4.1328  ]\n",
      " [ -7.6328  ]\n",
      " [ -2.1328  ]\n",
      " [ -9.1328  ]\n",
      " [ -1.8328  ]\n",
      " [ -3.5328  ]\n",
      " [  1.3672  ]\n",
      " [  3.8672  ]\n",
      " [  1.9672  ]\n",
      " [ -5.8328  ]\n",
      " [-13.033   ]\n",
      " [ -0.53281 ]\n",
      " [ -9.2328  ]\n",
      " [ -1.9328  ]\n",
      " [-11.233   ]\n",
      " [ 27.467   ]\n",
      " [  5.3672  ]\n",
      " [ -9.4328  ]\n",
      " [ -7.3328  ]\n",
      " [ -0.53281 ]\n",
      " [ 25.967   ]\n",
      " [ -1.4328  ]\n",
      " [-13.733   ]\n",
      " [ -3.4328  ]\n",
      " [ -8.7328  ]\n",
      " [ -3.3328  ]\n",
      " [ -2.5328  ]\n",
      " [ -0.83281 ]\n",
      " [ -4.1328  ]\n",
      " [  0.067194]\n",
      " [  1.2672  ]\n",
      " [ -0.33281 ]\n",
      " [ 12.167   ]\n",
      " [ -4.0328  ]\n",
      " [ 10.867   ]\n",
      " [ -9.9328  ]\n",
      " [  7.5672  ]\n",
      " [  5.3672  ]\n",
      " [ -2.7328  ]\n",
      " [ -3.2328  ]\n",
      " [ -1.7328  ]\n",
      " [ -2.9328  ]\n",
      " [ -5.7328  ]\n",
      " [  2.4672  ]\n",
      " [  8.5672  ]\n",
      " [  1.8672  ]\n",
      " [  0.26719 ]\n",
      " [ -1.3328  ]\n",
      " [ -7.5328  ]\n",
      " [  1.5672  ]\n",
      " [ -2.1328  ]\n",
      " [  0.56719 ]\n",
      " [ 27.467   ]\n",
      " [ 14.467   ]\n",
      " [ -9.8328  ]\n",
      " [ -1.5328  ]\n",
      " [ -0.63281 ]\n",
      " [ 12.867   ]\n",
      " [ -2.9328  ]\n",
      " [ -0.33281 ]\n",
      " [  0.76719 ]\n",
      " [ -1.1328  ]\n",
      " [ 13.667   ]\n",
      " [ -3.9328  ]\n",
      " [ -8.2328  ]\n",
      " [ 27.467   ]\n",
      " [ 27.467   ]\n",
      " [-11.633   ]\n",
      " [ -6.3328  ]\n",
      " [  5.8672  ]\n",
      " [  9.4672  ]\n",
      " [  7.5672  ]\n",
      " [ -3.8328  ]\n",
      " [ -1.6328  ]\n",
      " [ 27.467   ]\n",
      " [ 10.567   ]\n",
      " [  1.1672  ]\n",
      " [ 10.867   ]\n",
      " [ -2.2328  ]\n",
      " [ 14.667   ]\n",
      " [ -2.3328  ]\n",
      " [ -5.8328  ]\n",
      " [ -8.4328  ]\n",
      " [ -3.8328  ]\n",
      " [ -6.0328  ]\n",
      " [ -8.1328  ]\n",
      " [ -9.3328  ]\n",
      " [ 12.067   ]\n",
      " [  0.36719 ]\n",
      " [  2.2672  ]\n",
      " [ -8.7328  ]\n",
      " [  2.4672  ]\n",
      " [ -3.0328  ]\n",
      " [ -0.83281 ]\n",
      " [  2.4672  ]\n",
      " [-13.833   ]\n",
      " [  0.067194]\n",
      " [-15.333   ]\n",
      " [ 21.267   ]\n",
      " [ -5.3328  ]\n",
      " [ -0.83281 ]\n",
      " [ 10.667   ]\n",
      " [ -9.1328  ]\n",
      " [ -0.53281 ]\n",
      " [ -4.2328  ]\n",
      " [  0.56719 ]\n",
      " [ -5.3328  ]\n",
      " [-15.033   ]\n",
      " [ -2.3328  ]\n",
      " [  1.1672  ]\n",
      " [  3.8672  ]\n",
      " [  2.4672  ]\n",
      " [  6.4672  ]\n",
      " [-12.133   ]\n",
      " [  0.067194]\n",
      " [ -0.33281 ]\n",
      " [ -7.5328  ]\n",
      " [  9.0672  ]\n",
      " [ -6.9328  ]\n",
      " [ 13.567   ]\n",
      " [  0.36719 ]\n",
      " [ -4.2328  ]\n",
      " [  1.8672  ]\n",
      " [ 27.467   ]\n",
      " [ -1.9328  ]\n",
      " [ -9.1328  ]\n",
      " [  0.46719 ]\n",
      " [ -1.1328  ]\n",
      " [  5.5672  ]\n",
      " [ 27.467   ]\n",
      " [  1.8672  ]\n",
      " [ -8.6328  ]\n",
      " [ 13.967   ]\n",
      " [  0.86719 ]\n",
      " [ -5.4328  ]\n",
      " [  1.1672  ]\n",
      " [ -6.2328  ]\n",
      " [ -1.7328  ]\n",
      " [ -8.4328  ]\n",
      " [  6.1672  ]\n",
      " [  2.4672  ]\n",
      " [ -2.8328  ]\n",
      " [-10.233   ]\n",
      " [ -8.1328  ]\n",
      " [-10.833   ]\n",
      " [ -4.3328  ]\n",
      " [ -9.7328  ]\n",
      " [ -1.5328  ]\n",
      " [ -0.63281 ]\n",
      " [ 24.167   ]\n",
      " [  6.8672  ]\n",
      " [-15.333   ]\n",
      " [-10.733   ]\n",
      " [ -6.4328  ]\n",
      " [ -2.5328  ]\n",
      " [  4.0672  ]\n",
      " [  3.9672  ]\n",
      " [ -5.7328  ]\n",
      " [ -5.1328  ]\n",
      " [  4.5672  ]\n",
      " [ -3.1328  ]\n",
      " [  1.7672  ]\n",
      " [-12.933   ]\n",
      " [ -3.6328  ]\n",
      " [ 15.067   ]\n",
      " [  9.4672  ]\n",
      " [  2.2672  ]\n",
      " [ -3.8328  ]\n",
      " [ -4.7328  ]\n",
      " [ -8.7328  ]\n",
      " [  1.0672  ]\n",
      " [ -8.3328  ]\n",
      " [  4.4672  ]]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505]\n"
     ]
    }
   ],
   "source": [
    "X = data.x\n",
    "Y = data.y\n",
    "#X = data[:,:-1]\n",
    "#Y = data[:, -1].reshape(-1,1)\n",
    "\n",
    "print(X)\n",
    "print(Y)\n",
    "\n",
    "ind = np.arange(X.shape[0])\n",
    "print(ind)\n",
    "#np.random.seed(666)\n",
    "#np.random.shuffle(ind)\n",
    "prop = 0.9\n",
    "n = int(X.shape[0] * prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daa2f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch as gp\n",
    "from gpytorch_lattice_kernel import RBFLattice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "914c0567",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplexGPModel(gp.models.ExactGP):\n",
    "  def __init__(self, train_x, train_y):\n",
    "    likelihood = gp.likelihoods.GaussianLikelihood()\n",
    "    super().__init__(train_x, train_y, likelihood)\n",
    "\n",
    "    self.mean_module = gp.means.ConstantMean()\n",
    "    self.covar_module = gp.kernels.ScaleKernel(\n",
    "       RBFLattice(ard_num_dims=train_x.size(-1), order=1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    mean_x = self.mean_module(x)\n",
    "    covar_x = self.covar_module(x)\n",
    "    return gp.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8431a07c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 of 20\n",
      "Discretized kernel coeffs: tensor([0.3461, 1.0000, 0.3461])\n",
      "Discretized kernel deriv coeffs: tensor([0.3461, 1.0000, 0.3461])\n",
      "Using /home/martinj/.cache/torch_extensions/py38_cu102 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/martinj/.cache/torch_extensions/py38_cu102/cpu_lattice/build.ninja...\n",
      "Building extension module cpu_lattice...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_lattice...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martinj/simplex-gp/lib/python3.8/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:130: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\n",
      "X = torch.triangular_solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1672.)\n",
      "  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Loss: 1.0890693664550781\n",
      "Step: 2 Loss: 1.0003623962402344\n",
      "Step: 4 Loss: 0.9142631888389587\n",
      "Step: 6 Loss: 0.8340057134628296\n",
      "Step: 8 Loss: 0.7942389249801636\n",
      "Step: 10 Loss: 0.7357805967330933\n",
      "Step: 12 Loss: 0.6673582196235657\n",
      "Step: 14 Loss: 0.6382626295089722\n",
      "Step: 16 Loss: 0.5970591306686401\n",
      "Step: 18 Loss: 0.5821237564086914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martinj/simplex-gp/lib/python3.8/site-packages/gpytorch/utils/cholesky.py:38: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "/home/martinj/simplex-gp/lib/python3.8/site-packages/gpytorch/utils/cholesky.py:38: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n",
      "/home/martinj/simplex-gp/lib/python3.8/site-packages/gpytorch/utils/cholesky.py:38: NumericalWarning: A not p.d., added jitter of 1.0e-04 to the diagonal\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NotPSDError",
     "evalue": "Matrix not positive definite after repeatedly adding jitter up to 1.0e-04.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotPSDError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ITERATIONS): \n\u001b[0;32m---> 60\u001b[0m     l \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStep:\u001b[39m\u001b[38;5;124m'\u001b[39m, step, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss:\u001b[39m\u001b[38;5;124m'\u001b[39m, l)\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(x, y, model, mll, optim, lanc_iter, cg_iter, pre_size, cg_tol)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gp\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mcg_tolerance(cg_tol), \\\n\u001b[1;32m     48\u001b[0m    gp\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mmax_cg_iterations(cg_iter), \\\n\u001b[1;32m     49\u001b[0m    gp\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mmax_preconditioner_size(pre_size), \\\n\u001b[1;32m     50\u001b[0m    gp\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mmax_root_decomposition_size(lanc_iter):\n\u001b[1;32m     52\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m---> 53\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mmll\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     55\u001b[0m     optim\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/simplex-gp/lib/python3.8/site-packages/gpytorch/module.py:30\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 30\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/simplex-gp/lib/python3.8/site-packages/gpytorch/mlls/exact_marginal_log_likelihood.py:62\u001b[0m, in \u001b[0;36mExactMarginalLogLikelihood.forward\u001b[0;34m(self, function_dist, target, *params)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Get the log prob of the marginal distribution\u001b[39;00m\n\u001b[1;32m     61\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihood(function_dist, \u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m---> 62\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_other_terms(res, params)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Scale by the amount of data we have\u001b[39;00m\n",
      "File \u001b[0;32m~/simplex-gp/lib/python3.8/site-packages/gpytorch/distributions/multivariate_normal.py:169\u001b[0m, in \u001b[0;36mMultivariateNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# Get log determininant and first part of quadratic form\u001b[39;00m\n\u001b[1;32m    168\u001b[0m covar \u001b[38;5;241m=\u001b[39m covar\u001b[38;5;241m.\u001b[39mevaluate_kernel()\n\u001b[0;32m--> 169\u001b[0m inv_quad, logdet \u001b[38;5;241m=\u001b[39m \u001b[43mcovar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv_quad_logdet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minv_quad_rhs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogdet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m([inv_quad, logdet, diff\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mpi)])\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/simplex-gp/lib/python3.8/site-packages/gpytorch/lazy/lazy_tensor.py:1291\u001b[0m, in \u001b[0;36mLazyTensor.inv_quad_logdet\u001b[0;34m(self, inv_quad_rhs, logdet, reduce_inv_quad)\u001b[0m\n\u001b[1;32m   1289\u001b[0m             will_need_cholesky \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m will_need_cholesky:\n\u001b[0;32m-> 1291\u001b[0m         cholesky \u001b[38;5;241m=\u001b[39m CholLazyTensor(TriangularLazyTensor(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cholesky\u001b[38;5;241m.\u001b[39minv_quad_logdet(\n\u001b[1;32m   1293\u001b[0m         inv_quad_rhs\u001b[38;5;241m=\u001b[39minv_quad_rhs,\n\u001b[1;32m   1294\u001b[0m         logdet\u001b[38;5;241m=\u001b[39mlogdet,\n\u001b[1;32m   1295\u001b[0m         reduce_inv_quad\u001b[38;5;241m=\u001b[39mreduce_inv_quad,\n\u001b[1;32m   1296\u001b[0m     )\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;66;03m# Default: use modified batch conjugate gradients to compute these terms\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;66;03m# See NeurIPS 2018 paper: https://arxiv.org/abs/1809.11165\u001b[39;00m\n",
      "File \u001b[0;32m~/simplex-gp/lib/python3.8/site-packages/gpytorch/lazy/lazy_tensor.py:1004\u001b[0m, in \u001b[0;36mLazyTensor.cholesky\u001b[0;34m(self, upper)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcholesky\u001b[39m(\u001b[38;5;28mself\u001b[39m, upper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;124;03m    Cholesky-factorizes the LazyTensor\u001b[39;00m\n\u001b[1;32m    997\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;124;03m        (LazyTensor) Cholesky factor (triangular, upper/lower depending on \"upper\" arg)\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1004\u001b[0m     chol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1005\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m upper:\n\u001b[1;32m   1006\u001b[0m         chol \u001b[38;5;241m=\u001b[39m chol\u001b[38;5;241m.\u001b[39m_transpose_nonbatch()\n",
      "File \u001b[0;32m~/simplex-gp/lib/python3.8/site-packages/gpytorch/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/simplex-gp/lib/python3.8/site-packages/gpytorch/lazy/lazy_tensor.py:435\u001b[0m, in \u001b[0;36mLazyTensor._cholesky\u001b[0;34m(self, upper)\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TriangularLazyTensor(evaluated_mat\u001b[38;5;241m.\u001b[39mclamp_min(\u001b[38;5;241m0.0\u001b[39m)\u001b[38;5;241m.\u001b[39msqrt())\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# contiguous call is necessary here\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m cholesky \u001b[38;5;241m=\u001b[39m \u001b[43mpsd_safe_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluated_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupper\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m TriangularLazyTensor(cholesky, upper\u001b[38;5;241m=\u001b[39mupper)\n",
      "File \u001b[0;32m~/simplex-gp/lib/python3.8/site-packages/gpytorch/utils/cholesky.py:63\u001b[0m, in \u001b[0;36mpsd_safe_cholesky\u001b[0;34m(A, upper, out, jitter, max_tries)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpsd_safe_cholesky\u001b[39m(A, upper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, jitter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_tries\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute the Cholesky decomposition of A. If A is only p.s.d, add a small jitter to the diagonal.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m        :attr:`A` (Tensor):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m            Number of attempts (with successively increasing jitter) to make before raising an error.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     L \u001b[38;5;241m=\u001b[39m \u001b[43m_psd_safe_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjitter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjitter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m upper:\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/simplex-gp/lib/python3.8/site-packages/gpytorch/utils/cholesky.py:45\u001b[0m, in \u001b[0;36m_psd_safe_cholesky\u001b[0;34m(A, out, jitter, max_tries)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39many(info):\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m L\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m NotPSDError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatrix not positive definite after repeatedly adding jitter up to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjitter_new\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotPSDError\u001b[0m: Matrix not positive definite after repeatedly adding jitter up to 1.0e-04."
     ]
    }
   ],
   "source": [
    "ITERATIONS = 100\n",
    "\n",
    "SGPRRMSE = np.zeros(20)\n",
    "SGPRRMedSE = np.zeros(20)\n",
    "SGPRTestLL = np.zeros(20)\n",
    "\n",
    "for i in range(20):\n",
    "    print('Split', i + 1, 'of', 20)\n",
    "    np.random.seed(666 + i) # The devil's seed\n",
    "    np.random.shuffle(ind)\n",
    "    \n",
    "    ### GET TRAINING SPLIT AND PREPROCESS ###\n",
    "    \n",
    "    Xtrain = X[ind[:n], :]; Ytrain = Y[ind[:n], :]\n",
    "    Xtest = X[ind[n:], :]; Ytest = Y[ind[n:], :]\n",
    "    \n",
    "    mi, ma = Xtrain.min(axis=0), Xtrain.max(axis=0)\n",
    "    mami = ma - mi\n",
    "    mami = np.where(mami > 1e-6, mami, 1.) # Don't divide by zero\n",
    "    Xtrain = (Xtrain - mi) / mami\n",
    "    Xtest = (Xtest - mi) / mami\n",
    "\n",
    "    mi = 0.01\n",
    "    ma = 0.99\n",
    "    mami2 = np.where(mami > 1e-6, mami, 0.)\n",
    "    Xtrain = Xtrain * (ma - mi) + mi\n",
    "    Xtest = Xtest * (ma - mi) + mi\n",
    "\n",
    "    meany, stdy = np.average(Ytrain), np.std(Ytrain)\n",
    "    Ytrain = (Ytrain - meany) / stdy\n",
    "    \n",
    "    ############## SGPR ##################\n",
    "    \n",
    "    Xtrain = torch.tensor(Xtrain).to(torch.float32)\n",
    "    Ytrain = torch.tensor(Ytrain).squeeze().to(torch.float32)\n",
    "    m = SimplexGPModel(Xtrain, Ytrain)\n",
    "    \n",
    "    m.train()\n",
    "    optimizer = torch.optim.Adam(m.parameters(), lr=0.1)\n",
    "    mll = gp.mlls.ExactMarginalLogLikelihood(m.likelihood, m)\n",
    "    \n",
    "    def train(x, y, model, mll, optim, lanc_iter=100, cg_iter=500, pre_size=100, cg_tol=1e-2):\n",
    "        m.train()\n",
    "\n",
    "        optim.zero_grad()\n",
    "\n",
    "        with gp.settings.cg_tolerance(cg_tol), \\\n",
    "           gp.settings.max_cg_iterations(cg_iter), \\\n",
    "           gp.settings.max_preconditioner_size(pre_size), \\\n",
    "           gp.settings.max_root_decomposition_size(lanc_iter):\n",
    "\n",
    "            output = model(x)\n",
    "            loss = -mll(output, y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "    for step in range(ITERATIONS): \n",
    "        l = train(Xtrain, Ytrain, m, mll, optimizer)\n",
    "        if step % 2 == 0:\n",
    "            print('Step:', step, 'Loss:', l)\n",
    "\n",
    "    Xtest = torch.tensor(Xtest).to(torch.float32)\n",
    "    Ytest = torch.tensor(Ytest).squeeze().to(torch.float32)\n",
    "    \n",
    "    m.eval()\n",
    "    with gp.settings.cg_tolerance(1e-4), \\\n",
    "           gp.settings.max_cg_iterations(1000), \\\n",
    "           gp.settings.max_preconditioner_size(100), \\\n",
    "           gp.settings.max_root_decomposition_size(100):\n",
    "        pred_y = m(Xtest)\n",
    "    \n",
    "    pred = meany + stdy*pred_y.mean\n",
    "    \n",
    "    RMSE = (pred - Ytest).pow(2).mean(0).sqrt()\n",
    "    RMedSE = (pred - Ytest).pow(2).median(0).values.sqrt()\n",
    "    loglik = torch.distributions.Normal(pred,\n",
    "        pred_y.variance.add(m.likelihood.noise).sqrt() * stdy).log_prob(Ytest).mean()\n",
    "    \n",
    "    print('RMSE:', RMSE.item())\n",
    "    print('RMedSE:', RMedSE.item())\n",
    "    print('Test log-likelihood:', loglik.item())\n",
    "    \n",
    "    ### Append results\n",
    "    SGPRRMSE[i] = RMSE.item()\n",
    "    SGPRRMedSE[i] = RMedSE.item()\n",
    "    SGPRTestLL[i] = loglik.item()\n",
    "    \n",
    "print('Experiment concluded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a53509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataset:', data_name)\n",
    "print('Average RMSE:', np.mean(SGPRRMSE), 'Standard deviation:', np.std(SGPRRMSE))\n",
    "print('Average RMedSE:', np.mean(SGPRRMedSE), 'Standard deviation:', np.std(SGPRRMedSE))\n",
    "print('Average test-ll:', np.mean(SGPRTestLL), 'Standard deviation:', np.std(SGPRTestLL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c243b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.eval()\n",
    "print(m.likelihood.noise)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
